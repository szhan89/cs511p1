#!/bin/bash

####################################################################################
# DO NOT MODIFY THE BELOW ##########################################################

# Exchange SSH keys.
/etc/init.d/ssh start
eval "$(ssh-agent -s)"
ssh-add ~/.ssh/shared_rsa
ssh-copy-id -i ~/.ssh/id_rsa -o 'IdentityFile ~/.ssh/shared_rsa' -o StrictHostKeyChecking=no -f worker1
ssh-copy-id -i ~/.ssh/id_rsa -o 'IdentityFile ~/.ssh/shared_rsa' -o StrictHostKeyChecking=no -f worker2

# DO NOT MODIFY THE ABOVE ##########################################################
####################################################################################

# Start HDFS/Spark main here
#make sure all nodes are stoped
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root

$HADOOP_HOME/sbin/stop-dfs.sh
#rm -rf /opt/hadoop/data/namenode/dir/*
#rm -rf /opt/hadoop/data/datanode/dir/*

#format main for the first time use
$HADOOP_HOME/bin/hdfs namenode -format

$HADOOP_HOME/sbin/start-dfs.sh 
#in case these node did not start 
#$HADOOP_HOME/bin/hdfs datanode 
#$HADOOP_HOME/bin/hdfs secondarynamenode

# Start Spark
$SPARK_HOME/sbin/start-all.sh

bash
